{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FBXWvY7w8Kd",
        "outputId": "22bcabaf-aecd-4594-fea6-e53a3e70f74b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement zipfile (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for zipfile\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU openai pandas scikit-learn matplotlib requests zipfile python-dotenv sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOmZUXI5xTGc",
        "outputId": "0099f31f-f3d9-42c0-afc5-4ddb8510b6a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'extracted_data' 폴더 삭제\n",
            "파일 정리 완료\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "files_to_remove = ['trace.zip', 'attack_data.zip']\n",
        "folders_to_remove = ['extracted_data']\n",
        "\n",
        "for file in os.listdir('.'):\n",
        "    if file.startswith('trace') and file.endswith('.zip'):\n",
        "        os.remove(file)\n",
        "        print(f\"'{file}' 파일 삭제\")\n",
        "    if file.startswith('attack_data') and file.endswith('.zip'):\n",
        "        os.remove(file)\n",
        "        print(f\"'{file}' 파일 삭제\")\n",
        "\n",
        "for folder in folders_to_remove:\n",
        "    if os.path.exists(folder):\n",
        "        shutil.rmtree(folder)\n",
        "        print(f\"'{folder}' 폴더 삭제\")\n",
        "\n",
        "print(\"파일 정리 완료\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "75-qRFtatd8q",
        "outputId": "48679014-5d72-4a24-8fbb-e5b1f52bc27e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Enter your OpenAI API Key: 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\n",
            "파일 업로드\n",
            "source file upload\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d8ba1c81-db94-46e1-91b3-e81277571a51\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d8ba1c81-db94-46e1-91b3-e81277571a51\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving anomaly_trace.zip to anomaly_trace (2).zip\n",
            "'anomaly_trace (2).zip' 파일의 압축을 해제했습니다.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import openai\n",
        "from google.colab import drive, files\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import shutil\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "try:\n",
        "    api_key = input(\"Enter your OpenAI API Key: \")\n",
        "    openai.api_key = api_key\n",
        "except Exception as e:\n",
        "    print(f\"API key configuration failed: {e}\")\n",
        "    raise SystemExit(\"API 키 설정 실패. 유효한 API 키를 입력하세요.\")\n",
        "\n",
        "print(\"파일 업로드\")\n",
        "extract_dir = \"extracted_data\"\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "print(\"source file upload\")\n",
        "uploaded = files.upload()\n",
        "zip_file_name = list(uploaded.keys())[0]\n",
        "with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "print(f\"'{zip_file_name}' 파일의 압축을 해제했습니다.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DnGWIT2zxhs4",
        "outputId": "7532bdd0-039f-431b-eb4d-e021dca4152e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "파일 'trace-74311a0d9cf55883b2ad52d6f9fa1021.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-04d33f4bb14bcfd5dac6ad42368dc7ed.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-fe02d0af3f57e7bf4944add1040e405e.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-1ce4196f39f3920ad8ccc05ee0b667dc.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-313029a3c4d20fd154d01821210ab800.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-2dc3df71956c131217219fca1b2cd48a.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-695ae533d679d7989c8c55a9c14ebd9a.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-97cec519e8e4dc352da5f8e61c59b1cc.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-74d9de4f960523a9422257a3b530f06b.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-ded6a08ebfad6c236cb4a1a1ade55b80.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-883ba82a553dbcdb18f6f597de888b7c.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-6e7832c1e85560152c311d6ac0cbf176.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-964f02b36eede6ca182f8109c66a45aa.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-be2fbace258088e20e72478630c2aea6.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-3135282ecc8bed56de9a50ec12bcdf9b.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-01dea7ebd8e835ae53a0718e686fd681.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-56aa4daa68dc00902c9e96ecbd367000.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-e7585b147debea8d4df0bb34881c9313.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-95f2e58ada720ef99ad0326b4c4e43d5.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-5bfd12f1e545759ff0e9f61a58354688.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-db54e935c2094b704950ec55b83b6767.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-bf38a58c36dd6f08077b078f15ebfb22.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-a6283171a403d8a454673db33b573686.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-768100274a8fde818b31f26845c4724d.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-b861108f4a2b8421a5184bfd113bfa1e.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-7cc0329413e840ca32622079681ae462.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-9e4b609b766ee8c751d61277fa622b13.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-3081299db6011173e7c3850ecfe6f8db.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-38c82e633829e23d160ae879ea50436a.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-bf9d5163f49b01568ed6525a03e86bf6.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-1c48329975f65b4201be2ec361b918d8.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-ff223bc53d2e74596280f5e0387ae21d.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-8ec382694dec1d9940fae8c93a4095ac.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-d13667e8e393c6e824da6894abd66ffd.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-5e22cfa36502395adb0d7203b365f832.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-441c222fde51c54b8bae6f4bc09c239b.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-192b9e2cc51235721c21be58946aea60.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-a9ef0c13ca98a15d713ba14a0ec17bab.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-c315b499fc55e11e85e84435dc69aecb.json' 처리 중...\n",
            "Summarizing chunk 1/2...\n",
            "Summarizing chunk 2/2...\n",
            "파일 'trace-b36fbd9f78df7ffd3f4324377e2bca65.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-2d670571cd661df864e8910b954c7b1f.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-828ee470b92293ee5c2d77625016d8ba.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-76afd0bf468aea76be0a07a90c9c4446.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-5177866925e4c9456ba85322c9dae582.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-254d8f2b3eb0e9b4c050f67d26aba66c.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-499453e81f9f521d3238f27cedc42c03.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-40995644f4dfb1e15f58bb21044c6408.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-f4e1d98ce0a5fae404c7479a58bf526c.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-4755579b0a98c9acc243fffd9d94ae0f.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-5c80f6dfcb92d4d28fad15e4ba89f097.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-8bbd4478dbff006c74c824083219fd83.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-2bf418227fa20507117685739ecd70c7.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-0d2c8fc6741f46bcfd303c4adc2487a7.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-9e307c9fe30ff1884ed0965ee3caf549.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-26b5c4f5122c58f44866c235002d3f49.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-056fe3ab2c78f168a450dffcefbb219a.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-e2c3e45a63e2573cff6bb0eb4e62435d.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-50cd344b1defad5f90872ee7d88a875b.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-554ea0ec49677de743bfa40dacf9b3b4.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-09caf499d042fcc1aae1ac8ab8a0cc00.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-842385f8b4330d09418d565081192ce2.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-6ac9f28f6194bd1550dc4cd4399f4a61.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-63a2590bdc25f0f3dfe2e94256f423f2.json' 처리 중...\n",
            " 파일 내용이 너무 짧거나 노이즈로 판단되어 건너뜁니다.\n",
            "파일 'trace-a626b801c8187573dd6795821b142966.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-cb3acf3ba59abdac0213d2fd4975433c.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-e9405959f33e7b5ebce96309719ce365.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-4da73f21f27aee29db72b6e0e27871c2.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-b7bb10242aa0a1403d3b6194ebc04f62.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-683745a2baef7f98650a62a6f1124491.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-274ee0ac670cd21664042389114293d6.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-7f6e47bef2e237372a9d768b21b8703a.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-b45526f23888fbda5b9a8644b657ef8e.json' 처리 중...\n",
            "Summarizing chunk 1/4...\n",
            "Summarizing chunk 2/4...\n",
            "Summarizing chunk 3/4...\n",
            "Summarizing chunk 4/4...\n",
            "파일 'trace-04ab1e25240beff6690b353dbd94badd.json' 처리 중...\n",
            "Summarizing chunk 1/3...\n",
            "Summarizing chunk 2/3...\n",
            "Summarizing chunk 3/3...\n",
            "파일 'trace-0fc5cc35612975ca7bdda7b1e7a29a94.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-e5dc2d880d2300c947d272500bfa6950.json' 처리 중...\n",
            " 파일 내용이 너무 짧거나 노이즈로 판단되어 건너뜁니다.\n",
            "파일 'trace-8727c547e4ce5e26e0b7363adaa22778.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-7346a14c8c929657e1f8f4785c6be36c.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-7abd81cc2de1f17a7d1a84be80e62c26.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-834a1729d3ccda7f7659f63a7edb0c51.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-82f48656d023ddb2245de3d1299810f8.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-e13e5508626b548e2780b95ae3669139.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-ab4eb9e22429afdbdf9c3d3059808704.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-87ec45c98d15b2c895d7e2f9544ba80f.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-5dfc437f19271769f3433d38eb0fba85.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-fd20a9b23a51f578ad7f4c798ff60f3b.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-3a4394cdfe53b8cdab4bd136d1055efb.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-066359dfbf8860ba036de8c821b5c7d8.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-c4bbdd0dfe973ac1d2ec964269d501c7.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-7d01bf1581f625ae25efed74f97d49a2.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-94e5263336639a758acd1ad285aa736e.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-4ddf43c0603a17dda5f9e07a0b704b12.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-f77915e45b8483ca4bff304c5d0c9d75.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-feff8137f97aa2e21e6c4c5b19cc6154.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-706cfd342b1ca6425845e8f079709b4c.json' 처리 중...\n",
            " 파일 내용이 너무 짧거나 노이즈로 판단되어 건너뜁니다.\n",
            "파일 'trace-51b16b6215699cd6cff5129ccf9556ee.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-614320d41615d60031facae8b099fbab.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-b97c2a76a0407b86ceab5531ac9cd9b3.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-d5801f2c7ce1733032422cdac25fae2a.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-faf59cd7cf67c3443d4a75e2a25fe318.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-f7e738b66cce95426b1365786b5437a2.json' 처리 중...\n",
            "Summarizing chunk 1/2...\n",
            "Summarizing chunk 2/2...\n",
            "파일 'trace-f6730ccfe7d4bde6e0fd65a68d16ccf8.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-3b3921e8d737d0a4975d6044d72044a7.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-cdc76e000099771707b857e3e290bd61.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-ff0f7637b755e148cc533e409ba70258.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-69843a3810ec4da72be0928796ab3dd2.json' 처리 중...\n",
            "Summarizing chunk 1/4...\n",
            "Summarizing chunk 2/4...\n",
            "Summarizing chunk 3/4...\n",
            "Summarizing chunk 4/4...\n",
            "파일 'trace-d6eaf4ee61f01ff116d311b5d6a8ad8c.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-311874824d8e94eb421e9c4dad154bfd.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-2895ba9af6bdb2b634be3fbed31aaaf7.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-2a33acd7880e03c9d1eb116f518113a7.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-ffb7b5cd87413e4d51c2156ab6e8a31a.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-5fdc94c9fa3f83ad40f5b8742dbda4a6.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-117bbc69a89ee84643e59f40ebe1a881.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-776a2b314c39bbf6bd641c1ea02f217c.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-a0001a8efe5764d3f03c9a710f47be36.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-843deb7919e71f5192c5af27afe20e7d.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-682fb135d1802a2b96088e6d48305f45.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-f9716548156f55ec098ecf30ae8beecb.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-d1495988bb29d5fe620ada281a79a66c.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-60086a62e244607513e9467f424e30b5.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-aa8bb38388a4a3cc7c118fd234d9c5bf.json' 처리 중...\n",
            " 파일 내용이 너무 짧거나 노이즈로 판단되어 건너뜁니다.\n",
            "파일 'trace-f3018dee8ec814caae05f160eab0b447.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n",
            "파일 'trace-4e89c160bf02433ab71c27acf6eecac5.json' 처리 중...\n",
            "Summarizing chunk 1/1...\n"
          ]
        }
      ],
      "source": [
        "# 데이터 전처리 및 요약\n",
        "\n",
        "def extract_evidence_string(spans):\n",
        "  evidence_pieces = []\n",
        "  for span in spans:\n",
        "        tags = {tag['key']: tag['value'] for tag in span.get('tags', [])}\n",
        "        evidence_pieces.append(str(tags.get(\"EventName\", \"\")))\n",
        "        evidence_pieces.append(str(tags.get(\"Image\", \"\")))\n",
        "        evidence_pieces.append(str(tags.get(\"CommandLine\", \"\")))\n",
        "        evidence_pieces.append(str(tags.get(\"QueryName\", \"\")))\n",
        "        evidence_pieces.append(str(tags.get(\"sigma.rule_title\", \"\")))\n",
        "        evidence_pieces.append(str(tags.get(\"ParentImage\", \"\")))\n",
        "\n",
        "    # 공백 기준으로 띄어쓴 후 구분\n",
        "  return \" \".join(filter(None, evidence_pieces))\n",
        "\n",
        "def preprocess_and_summarize_chunk(spans_chunk):\n",
        "    preprocessed_spans = []\n",
        "    for span in spans_chunk:\n",
        "        tags = {tag['key']: tag['value'] for tag in span.get('tags', [])}\n",
        "        preprocessed_spans.append({\n",
        "            \"event_name\": tags.get(\"EventName\"),\n",
        "            \"image\": tags.get(\"Image\"),\n",
        "            \"parent_image\": tags.get(\"ParentImage\"),\n",
        "            \"command_line\": tags.get(\"CommandLine\"),\n",
        "            \"query_name\": tags.get(\"QueryName\"),\n",
        "            \"sigma_rule\": tags.get(\"sigma.rule_title\"),\n",
        "            \"error\": tags.get(\"error\", False)\n",
        "        })\n",
        "\n",
        "    processed_content = json.dumps(preprocessed_spans, indent=2)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are a security expert. I will provide you with preprocessed security data in JSON format. Your task is to analyze this data and provide a concise summary. The summary should focus on key events, potential malicious activity, and relevant security indicators.\n",
        "    ---\n",
        "    {processed_content}\n",
        "    ---\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = openai.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a security expert.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.7\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"GPT API call failed: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# def summarize_file_with_gpt(file_content, file_ext): # 파일 내용 -> 스팬 100개씩 묶어 청크 하나로 생성, 각 청크를 자연어 요약 및 해당 자연어 요약을 임베딩함\n",
        "#     if file_ext == '.json':\n",
        "#         try:\n",
        "#             raw_data = json.loads(file_content)\n",
        "#             spans = raw_data.get('spans', [])\n",
        "\n",
        "#             chunk_size = 100\n",
        "#             chunks = [spans[i:i + chunk_size] for i in range(0, len(spans), chunk_size)]\n",
        "\n",
        "#             chunk_summaries = []\n",
        "#             for i, chunk in enumerate(chunks):\n",
        "#                 print(f\"  - Summarizing chunk {i+1}/{len(chunks)}...\")\n",
        "#                 chunk_summary = preprocess_and_summarize_chunk(chunk)\n",
        "#                 if chunk_summary:\n",
        "#                     chunk_summaries.append(f\"Summary of chunk {i+1}: {chunk_summary}\")\n",
        "\n",
        "#             if not chunk_summaries:\n",
        "#                 return \"\"\n",
        "\n",
        "#             combined_summary_prompt = \"\\n\".join(chunk_summaries)\n",
        "\n",
        "#             final_prompt = f\"\"\"\n",
        "#             You are a security expert. I have summarized different parts of a security trace file. Your task is to analyze these summaries and provide a single, comprehensive summary of the entire file in Korean. The summary should be concise, highlight potential malicious activity, and mention key events.\n",
        "#             ---\n",
        "#             {combined_summary_prompt}\n",
        "#             ---\n",
        "#             \"\"\"\n",
        "\n",
        "#             response = openai.chat.completions.create(\n",
        "#                 model=\"gpt-4o-mini\",\n",
        "#                 messages=[\n",
        "#                     {\"role\": \"system\", \"content\": \"You are a security expert.\"},\n",
        "#                     {\"role\": \"user\", \"content\": final_prompt}\n",
        "#                 ],\n",
        "#                 temperature=0.7\n",
        "#             )\n",
        "#             return response.choices[0].message.content\n",
        "\n",
        "#         except json.JSONDecodeError:\n",
        "#             return summarize_file_with_gpt_fallback(file_content)\n",
        "#     else:\n",
        "#         return summarize_file_with_gpt_fallback(file_content)\n",
        "\n",
        "def summarize_file_with_gpt_fallback(file_content):\n",
        "    prompt = f\"\"\"\n",
        "    You are a security expert. Analyze the following log or text file content and provide a concise summary in Korean. Highlight key security events and potential malicious indicators.\n",
        "    ---\n",
        "    {file_content[:10000]}\n",
        "    ---\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = openai.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a security expert.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.7\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"GPT API call failed: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "def summarize_and_extract_evidence(file_content, file_ext):\n",
        "    if file_ext == '.json':\n",
        "        try:\n",
        "            raw_data = json.loads(file_content)\n",
        "            spans = raw_data.get('spans', [])\n",
        "\n",
        "            evidence_string = extract_evidence_string(spans)\n",
        "\n",
        "            # 자연어 요약 생성\n",
        "            chunk_size = 100\n",
        "            chunks = [spans[i:i + chunk_size] for i in range(0, len(spans), chunk_size)]  # 청크 사이즈 -> 스팬 100개 단위\n",
        "\n",
        "            chunk_summaries = []\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                print(f\"Summarizing chunk {i+1}/{len(chunks)}...\")\n",
        "                chunk_summary = preprocess_and_summarize_chunk(chunk)\n",
        "                if chunk_summary:\n",
        "                    chunk_summaries.append(f\"Summary of chunk {i+1}: {chunk_summary}\")\n",
        "\n",
        "            if not chunk_summaries:\n",
        "                # 요약에 실패하더라도 -> 증거 문자열은 반환\n",
        "                return \"No summary generated.\", evidence_string\n",
        "\n",
        "            combined_summary_prompt = \"\\n\".join(chunk_summaries)\n",
        "\n",
        "            final_prompt = f\"\"\"\n",
        "            You are a security expert. I have summarized different parts of a security trace file. Your task is to analyze these summaries and provide a single, comprehensive summary of the entire file in Korean. The summary should be concise, highlight potential malicious activity, and mention key events.\n",
        "            ---\n",
        "            {combined_summary_prompt}\n",
        "            ---\n",
        "            \"\"\"\n",
        "\n",
        "            response = openai.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a security expert.\"},\n",
        "                    {\"role\": \"user\", \"content\": final_prompt}\n",
        "                ],\n",
        "                temperature=0.7\n",
        "            )\n",
        "            final_summary = response.choices[0].message.content\n",
        "\n",
        "            # 자연어 요약과 원본 증거 문자열을 모두 반환\n",
        "            return final_summary, evidence_string\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            # JSON 파싱 실패 -> fallback 요약 사용 및 원본 텍스트 증거 사용\n",
        "            print(\"  - JSON Decode Error. Using fallback summarizer.\")\n",
        "            final_summary = summarize_file_with_gpt_fallback(file_content)\n",
        "            evidence_string = file_content[:10000] # 잘린 원본 텍스트를 증거로 사용\n",
        "            return final_summary, evidence_string\n",
        "    else:\n",
        "        # json이 아닐 경우\n",
        "        print(f\" Non-JSON file. Using fallback summarizer.\")\n",
        "        final_summary = summarize_file_with_gpt_fallback(file_content)\n",
        "        evidence_string = file_content[:10000] # 잘린 원본 텍스트를 증거로 사용\n",
        "        return final_summary, evidence_string\n",
        "\n",
        "\n",
        "all_file_data = []\n",
        "\n",
        "# 노이즈 데이터 필터링 기준\n",
        "EXCLUDE_KEYWORDS = ['Windows Defender', 'Sysmon service']  # 기본적인 방화벽 시스템 / sysmon 작동은 일단 제외\n",
        "# 블랙리스트는 나중에 더 추가\n",
        "for root, _, files in os.walk(extract_dir):\n",
        "    for file in files:\n",
        "        file_path = os.path.join(root, file)\n",
        "\n",
        "        file_ext = os.path.splitext(file)[1].lower()\n",
        "\n",
        "        print(f\"파일 '{file}' 처리 중...\")\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8-sig') as f:\n",
        "                file_content = f.read()\n",
        "\n",
        "                # 노이즈 데이터 필터링\n",
        "                if any(kw in file_content for kw in EXCLUDE_KEYWORDS) or len(file_content) < 100:\n",
        "                    print(f\" 파일 내용이 너무 짧거나 노이즈로 판단되어 건너뜁니다.\")\n",
        "                    continue\n",
        "\n",
        "                summary_text, evidence_text = summarize_and_extract_evidence(file_content, file_ext)\n",
        "\n",
        "                if summary_text and evidence_text: # 둘 다 true\n",
        "                    all_file_data.append({\n",
        "                        'file_path': file_path,\n",
        "                        'summary': summary_text,\n",
        "                        'evidence': evidence_text # 클러스터링 용 전처리 데이터\n",
        "                    })\n",
        "                elif summary_text:\n",
        "                  print(f\"증거 문자열은 비어있지만 요약문은 생성됨\")\n",
        "                  all_file_data.append({\n",
        "                      'file_path': file_path,\n",
        "                      'summary': summary_text,\n",
        "                      'evidence': \"\" # 클러스터링 용 전처리 데이터\n",
        "                  })\n",
        "                else:\n",
        "                  print(f\" 증거 추출 실패 \")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"파일 '{file_path}' 처리 중 오류 발생: {e}\")\n",
        "\n",
        "if not all_file_data:\n",
        "    raise SystemExit(\"오류: 요약할 데이터가 없습니다. 파일 내용을 확인하거나 경로를 검토해 주세요.\")\n",
        "\n",
        "print(f\"\\n총 {len(all_file_data)}개의 파일 처리 완료.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EYkkLT5xkMG"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "# 임베딩, 클러스터링 및 시각화\n",
        "def embed_with_sbert(text_list):\n",
        "  try:\n",
        "    model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
        "    embeddings = model.encode(text_list, show_progress_bar = True)\n",
        "    return embeddings.tolist()\n",
        "  except Exception as e:\n",
        "    print(f\"{e}\")\n",
        "    return []\n",
        "\n",
        "# embeddings -> 768 차원의 임베딩 벡터\n",
        "# 원본 매트릭스라고 생각하면됨\n",
        "\n",
        "# 요약된 텍스트 -> 임베딩\n",
        "print(f\"\\n총 {len(all_file_data)}개의 파일에 대한 원본 증거(Evidence) 문자열을 임베딩합니다.\")\n",
        "print(\" (생성된 자연어 요약본은 라벨링 및 결과 확인에만 사용됩니다.)\")\n",
        "evidence_list = [item['evidence'] for item in all_file_data] # 임베딩 대상을 'evidence'로 변경\n",
        "embeddings_list = embed_with_sbert(evidence_list)\n",
        "\n",
        "if not embeddings_list:\n",
        "    raise SystemExit(\"오류: 임베딩 생성에 실패했습니다. API 키나 네트워크 상태를 확인해 주세요.\")\n",
        "\n",
        "# DataFrame에 임베딩 추가\n",
        "df_embeddings = pd.DataFrame(all_file_data)\n",
        "df_embeddings['embedding'] = embeddings_list\n",
        "\n",
        "print(f\"\\n총 {len(df_embeddings)}개의 파일 요약 및 임베딩을 완료했습니다.\")\n",
        "embeddings_matrix = np.array(df_embeddings['embedding'].tolist())\n",
        "\n",
        "\n",
        "\n",
        "# l2 정규화를 이용한 거릿값 구하기 -> 모든 벡터의 크기를 1로 만들어 평준화\n",
        "# 의미 -> 방향의 유사성이 더 중요함\n",
        "# K-Means의 유클리드 거리 -> 코사인 유사도 계산으로 변경\n",
        "print(\"임베딩 벡터 L2 정규화 수행\")\n",
        "transformer = Normalizer(norm='l2')\n",
        "norm_embeddings_matrix = transformer.fit_transform(embeddings_matrix)\n",
        "\n",
        "\n",
        "# 1단계 클러스터링: k=2로 고정하여 정상/악성으로 크게 나눔\n",
        "# 정규화된 matrix 행렬사용 -> l2 정규화를 거친 임베딩 벡터를 의미함\n",
        "kmeans_stage1 = KMeans(n_clusters=2, random_state=42, n_init='auto')\n",
        "stage1_labels = kmeans_stage1.fit_predict(norm_embeddings_matrix) # 라벨을 변수에 먼저 저장\n",
        "df_embeddings['cluster_label_stage1'] = stage1_labels # 라벨을 DF에 저장\n",
        "\n",
        "try:\n",
        "    # 실루엣 스코어 계산 시에도 정규화된 매트릭스와 해당 라벨 사용\n",
        "    stage1_score = silhouette_score(norm_embeddings_matrix, stage1_labels)\n",
        "    print(f\"\\n[1단계 클러스터링] 실루엣 스코어 (k=2): {stage1_score:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n[1단계 클러스터링] 실루엣 스코어 계산 오류: {e}\")\n",
        "\n",
        "# 1단계 클러스터링 시각화\n",
        "# pca는 분산(Variance)을 기반으로 하므로 정규화되지않은 원본 매트릭스를 사용함\n",
        "pca_stage1 = PCA(n_components=2) # 2차원 축소\n",
        "principal_components_stage1 = pca_stage1.fit_transform(embeddings_matrix)\n",
        "df_embeddings['pc1_stage1'] = principal_components_stage1[:, 0]\n",
        "df_embeddings['pc2_stage1'] = principal_components_stage1[:, 1]\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "scatter = plt.scatter(df_embeddings['pc1_stage1'], df_embeddings['pc2_stage1'], c=df_embeddings['cluster_label_stage1'], cmap='viridis', s=100)\n",
        "plt.title('Stage 1 Clustering: Benign vs. Malicious (Clustered on Evidence Embeddings)')\n",
        "plt.xlabel('Major Behavioral Differences')\n",
        "plt.ylabel('Secondary Behavioral Differences')\n",
        "plt.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
        "plt.grid(True)\n",
        "plt.savefig(\"stage1_clustering.png\")\n",
        "print(\"\\nStage 1 클러스터링 시각화 이미지를 'stage1_clustering.png'로 저장했습니다.\")\n",
        "\n",
        "\n",
        "# 2단계 클러스터링 -> 악성 행위 세부 분류\n",
        "# 라벨링 기준은 'summary' 필드를 사용 (사람이 읽는 요약 기준)\n",
        "malicious_cluster_id = df_embeddings['cluster_label_stage1'][\n",
        "    df_embeddings['summary'].str.contains('Sigma rule|악성|의심', case=False) # 키워드 추가\n",
        "].mode().iloc[0] if not df_embeddings['summary'].str.contains('Sigma rule|악성|의심', case=False).empty else None\n",
        "\n",
        "\n",
        "if malicious_cluster_id is not None:\n",
        "    malicious_data = df_embeddings[df_embeddings['cluster_label_stage1'] == malicious_cluster_id].copy()\n",
        "else:\n",
        "    print(\"오류: 악성 행위의 특징(Sigma rule 등)을 찾을 수 없습니다. 세부 클러스터링을 진행할 수 없습니다.\")\n",
        "    # 임시방편: 만약 시그마룰이 없다면, 그냥 더 작은 클러스터를 악성으로 간주\n",
        "    cluster_counts = df_embeddings['cluster_label_stage1'].value_counts()\n",
        "    if not cluster_counts.empty:\n",
        "        malicious_cluster_id = cluster_counts.idxmin()\n",
        "        malicious_data = df_embeddings[df_embeddings['cluster_label_stage1'] == malicious_cluster_id].copy()\n",
        "        print(f\"경고: 'Sigma rule'을 찾지 못해, 더 작은 클러스터 ({malicious_cluster_id})를 악성으로 간주하고 진행합니다.\")\n",
        "    else:\n",
        "         raise SystemExit(\"분석 중단: 클러스터링 데이터를 찾을 수 없습니다.\")\n",
        "\n",
        "\n",
        "if len(malicious_data) < 2:\n",
        "    print(f\"\\n악성 데이터가 2개 미만(총 {len(malicious_data)}개)이어서 세부 클러스터링을 진행할 수 없습니다.\")\n",
        "    raise SystemExit(\"분석 중단.\")\n",
        "\n",
        "# 악성 데이터 부분집합에 대해서도 원본 임베딩 추출 및 L2 정규화 수행\n",
        "malicious_embeddings_matrix_orig = np.array(malicious_data['embedding'].tolist())\n",
        "malicious_embeddings_matrix_norm = transformer.transform(malicious_embeddings_matrix_orig) # 기존 transformer로 정규화\n",
        "\n",
        "print(f\"\\n[3/5] 2단계 클러스터링: {len(malicious_data)}개의 악성 데이터를 세부 분류\")\n",
        "\n",
        "silhouette_scores_stage2 = []\n",
        "k_range_stage2 = range(2, min(11, len(malicious_embeddings_matrix_norm)))\n",
        "if len(k_range_stage2) > 0:\n",
        "    for k in k_range_stage2:\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
        "        # 실루엣 스코어 계산 시 정규화된(norm) 매트릭스 사용\n",
        "        cluster_labels = kmeans.fit_predict(malicious_embeddings_matrix_norm)\n",
        "        silhouette_avg = silhouette_score(malicious_embeddings_matrix_norm, cluster_labels)\n",
        "        silhouette_scores_stage2.append(silhouette_avg)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(k_range_stage2, silhouette_scores_stage2, marker='o')\n",
        "    plt.title('Silhouette Analysis for Malicious Data (on Normalized Evidence Embeddings)')\n",
        "    plt.xlabel('Number of clusters (k)')\n",
        "    plt.ylabel('Silhouette Score')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(\"stage2_silhouette.png\")\n",
        "    print(\"Stage 2 실루엣 분석 그래프를 'stage2_silhouette.png'로 저장했습니다.\")\n",
        "else:\n",
        "    print(\"실루엣 분석을 실행하기에 데이터가 부족합니다 (k range < 1).\")\n",
        "\n",
        "\n",
        "try:\n",
        "    optimal_k_stage2 = int(input(f\"\\n[4/5] 실루엣 그래프(stage2_silhouette.png)를 참고하여 최적의 k값을 입력하세요 (범위: {k_range_stage2.start}-{k_range_stage2.stop -1}): \"))\n",
        "    if optimal_k_stage2 not in k_range_stage2:\n",
        "        raise ValueError\n",
        "except (ValueError, IndexError, NameError):\n",
        "    default_k = 3 if 3 in k_range_stage2 else k_range_stage2.start if k_range_stage2 else 2\n",
        "    print(f\"잘못된 k값입니다. 기본값인 {default_k}(으)로 설정합니다.\")\n",
        "    optimal_k_stage2 = default_k\n",
        "\n",
        "print(f\"선택된 k값 {optimal_k_stage2}로 세부 클러스터링 실행\")\n",
        "kmeans_stage2 = KMeans(n_clusters=optimal_k_stage2, random_state=42, n_init='auto')\n",
        "# 최종 클러스터링도 정규화된(normalized) 행렬(matrix) 사용\n",
        "malicious_data['sub_cluster_label'] = kmeans_stage2.fit_predict(malicious_embeddings_matrix_norm)\n",
        "\n",
        "# 2단계 pca\n",
        "pca_stage2 = PCA(n_components=2)\n",
        "principal_components_stage2 = pca_stage2.fit_transform(malicious_embeddings_matrix_orig)\n",
        "malicious_data['pc1'] = principal_components_stage2[:, 0]\n",
        "malicious_data['pc2'] = principal_components_stage2[:, 1]\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "scatter = plt.scatter(malicious_data['pc1'], malicious_data['pc2'], c=malicious_data['sub_cluster_label'], cmap='viridis', s=100)\n",
        "plt.title(f'Malicious Activity Sub-Clustering (k={optimal_k_stage2})')\n",
        "plt.xlabel('주요 행위 패턴의 차이')\n",
        "plt.ylabel('보조 행위 패턴의 차이')\n",
        "plt.legend(*scatter.legend_elements(), title=\"Sub-Clusters\")\n",
        "plt.grid(True)\n",
        "plt.savefig(f\"stage2_sub_clustering_k{optimal_k_stage2}.png\") # 파일로 저장\n",
        "print(f\"Stage 2 세부 클러스터링 시각화(k={optimal_k_stage2}) 이미지를 'stage2_sub_clustering_k{optimal_k_stage2}.png'로 저장했습니다.\")\n",
        "\n",
        "\n",
        "# 악성 행위 세부 클러스터별 요약\n",
        "# 클러스터링 -> 'evidence'로 했지만, 요약은 읽기 쉬운 'summary'를 사용\n",
        "print(\"\\n[5/5] 최종 분석: 악성 행위 세부 클러스터별 요약 (자연어 요약 기준)\")\n",
        "for i in range(optimal_k_stage2):\n",
        "    sub_cluster_data = malicious_data[malicious_data['sub_cluster_label'] == i]\n",
        "    print(f\"\\n--- 세부 클러스터 {i} ({len(sub_cluster_data)} files) ---\")\n",
        "\n",
        "    # 요약문(summary)을 대표값으로 출력\n",
        "    representative_summaries = sub_cluster_data['summary'].head(5)\n",
        "    for summary in representative_summaries:\n",
        "        print(f\"\\n [대표 요약]: {summary}\")\n",
        "\n",
        "    print(\"\\n (위 요약에 해당하는 원본 파일 경로):\")\n",
        "    for f_path in sub_cluster_data['file_path'].head(5):\n",
        "        print(f\" - {f_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFGQEpprpbg5"
      },
      "outputs": [],
      "source": [
        "# ai 프로파일링을 통해 라벨링 검증\n",
        "\n",
        "print(f\"총 {optimal_k_stage2}개의 세부 클러스터에 대한 AI 분석 시작\")\n",
        "\n",
        "for i in range(optimal_k_stage2):\n",
        "    sub_cluster_data = malicious_data[malicious_data['sub_cluster_label'] == i]\n",
        "    print(f\"\\n==================== [ 세부 클러스터 {i} (총 {len(sub_cluster_data)}개 파일) ] ====================\")\n",
        "\n",
        "    # gpt에게 각 클러스터 별 대푯값 전송\n",
        "    representative_summaries = sub_cluster_data['summary'].head(5) # 대표 요약 5개 추출\n",
        "    prompt_summaries = \"\\n\\n\".join(representative_summaries) # 요약문들을 결합\n",
        "\n",
        "    ai_prompt = f\"\"\"\n",
        "    You are a cyber attack classifier and senior security analyst. I will provide up to 5 representative summaries from log file traces that were grouped together by a clustering algorithm. Analyze their common patterns and TTPs.\n",
        "\n",
        "    Based only on the text below, what specific attack type (e.g., Ransomware, Credential Dumping, Defense Evasion, Worm, Trojan) is this cluster most likely to be?\n",
        "\n",
        "    Explain your reasoning concisely in Korean.\n",
        "\n",
        "    --- [REPRESENTATIVE SUMMARIES] ---\n",
        "    {prompt_summaries}\n",
        "    --- [END SUMMARIES] ---\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n분석결과:\")\n",
        "    try:\n",
        "        response = openai.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a cyber attack classifier and security analyst.\"},\n",
        "                {\"role\": \"user\", \"content\": ai_prompt}\n",
        "            ],\n",
        "            temperature=0.7\n",
        "        )\n",
        "        ai_analysis = response.choices[0].message.content\n",
        "        print(ai_analysis)\n",
        "    except Exception as e:\n",
        "        print(f\"  - AI 분류 API 호출 실패: {e}\")\n",
        "        print(\"  - (참고) 대표 요약문 5개 원본:\")\n",
        "        print(prompt_summaries) # 호출 실패시 -> 원본 요약문 출력\n",
        "\n",
        "    # 원본 파일 경로 출력\n",
        "    print(\"\\n(참고: 위 분석에 사용된 대표 파일 경로 5개):\")\n",
        "    for f_path in sub_cluster_data['file_path'].head(5):\n",
        "        print(f\"\\n {f_path}\")\n",
        "print(\"ai 분석완료\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9As4AOYDPvZ0"
      },
      "outputs": [],
      "source": [
        "# 수정전\n",
        "# # 1단계 클러스터링: k=2로 고정하여 정상/악성으로 크게 나눔 (데이터 노이즈 후)\n",
        "# kmeans_stage1 = KMeans(n_clusters=2, random_state=42, n_init='auto')\n",
        "# df_embeddings['cluster_label_stage1'] = kmeans_stage1.fit_predict(embeddings_matrix)\n",
        "\n",
        "# # 1단계 클러스터링 결과 시각화\n",
        "# pca_stage1 = PCA(n_components=2)\n",
        "# principal_components_stage1 = pca_stage1.fit_transform(embeddings_matrix)\n",
        "# df_embeddings['pc1_stage1'] = principal_components_stage1[:, 0]\n",
        "# df_embeddings['pc2_stage1'] = principal_components_stage1[:, 1]\n",
        "\n",
        "# plt.figure(figsize=(12, 8))\n",
        "# scatter = plt.scatter(df_embeddings['pc1_stage1'], df_embeddings['pc2_stage1'], c=df_embeddings['cluster_label_stage1'], cmap='viridis', s=100)\n",
        "# plt.title('Stage 1 Clustering: Benign vs. Malicious')\n",
        "# plt.xlabel('Major Behavioral Differences')\n",
        "# plt.ylabel('Secondary Behavioral Differences')\n",
        "# plt.legend(*scatter.legend_elements(), title=\"Clusters\")\n",
        "# plt.grid(True)\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# # 2단계 클러스터링 -> 악성 행위 세부 분류\n",
        "# # 1단계 클러스터 결과 프로파일링 ('Sigma rule' 키워드로 악성 클러스터 식별)\n",
        "# malicious_cluster_id = df_embeddings['cluster_label_stage1'][\n",
        "#     df_embeddings['summary'].str.contains('Sigma rule', case=False)\n",
        "# ].mode().iloc[0] if not df_embeddings['summary'].str.contains('Sigma rule', case=False).empty else None\n",
        "\n",
        "# if malicious_cluster_id is not None:\n",
        "#     malicious_data = df_embeddings[df_embeddings['cluster_label_stage1'] == malicious_cluster_id].copy()\n",
        "# else:\n",
        "#     print(\"오류: 악성 행위의 특징(Sigma rule)을 찾을 수 없습니다. 세부 클러스터링을 진행할 수 없습니다.\")\n",
        "#     raise SystemExit(\"분석 중단.\")\n",
        "\n",
        "# if len(malicious_data) < 2:\n",
        "#     print(\"\\n악성 데이터가 2개 미만이어서 세부 클러스터링을 진행할 수 없습니다.\")\n",
        "#     raise SystemExit(\"분석 중단.\")\n",
        "\n",
        "# malicious_embeddings_matrix = np.array(malicious_data['embedding'].tolist())\n",
        "\n",
        "# print(f\"\\n[3/5] 2단계 클러스터링: {len(malicious_data)}개의 악성 데이터를 세부 분류\")\n",
        "\n",
        "\n",
        "# silhouette_scores_stage2 = []\n",
        "# k_range_stage2 = range(2, min(11, len(malicious_embeddings_matrix)))\n",
        "# if len(k_range_stage2) > 0:\n",
        "#     for k in k_range_stage2:\n",
        "#         kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
        "#         cluster_labels = kmeans.fit_predict(malicious_embeddings_matrix)\n",
        "#         silhouette_avg = silhouette_score(malicious_embeddings_matrix, cluster_labels)\n",
        "#         silhouette_scores_stage2.append(silhouette_avg)\n",
        "#     plt.figure(figsize=(10, 6))\n",
        "#     plt.plot(k_range_stage2, silhouette_scores_stage2, marker='o')\n",
        "#     plt.title('Silhouette Analysis for Malicious Data')\n",
        "#     plt.xlabel('Number of clusters (k)')\n",
        "#     plt.ylabel('Silhouette Score')\n",
        "#     plt.grid(True)\n",
        "#     plt.show()\n",
        "\n",
        "# try:\n",
        "#     optimal_k_stage2 = int(input(\"\\n[4/5] 악성 행위 세부 분류를 위한 최적의 k값을 입력하세요: \"))\n",
        "#     if optimal_k_stage2 < 2 or optimal_k_stage2 > len(malicious_data):\n",
        "#         raise ValueError\n",
        "# except (ValueError, IndexError):\n",
        "#     print(\"잘못된 k값입니다. 기본값인 3으로 설정합니다.\")\n",
        "#     optimal_k_stage2 = 3\n",
        "\n",
        "# print(f\"선택된 k값 {optimal_k_stage2}로 세부 클러스터링 실행\")\n",
        "# kmeans_stage2 = KMeans(n_clusters=optimal_k_stage2, random_state=42, n_init='auto')\n",
        "# malicious_data['sub_cluster_label'] = kmeans_stage2.fit_predict(malicious_embeddings_matrix)\n",
        "\n",
        "# # pca 2차원 축소\n",
        "# pca_stage2 = PCA(n_components=2)\n",
        "# principal_components_stage2 = pca_stage2.fit_transform(malicious_embeddings_matrix)\n",
        "# malicious_data['pc1'] = principal_components_stage2[:, 0]\n",
        "# malicious_data['pc2'] = principal_components_stage2[:, 1]\n",
        "\n",
        "# plt.figure(figsize=(12, 8))\n",
        "# scatter = plt.scatter(malicious_data['pc1'], malicious_data['pc2'], c=malicious_data['sub_cluster_label'], cmap='viridis', s=100)\n",
        "# plt.title(f'Malicious Activity Sub-Clustering (k={optimal_k_stage2})')\n",
        "# plt.xlabel('주요 행위 패턴의 차이')\n",
        "# plt.ylabel('보조 행위 패턴의 차이')\n",
        "# plt.legend(*scatter.legend_elements(), title=\"Sub-Clusters\")\n",
        "# plt.grid(True)\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# print(\"\\n[5/5] 최종 분석: 악성 행위 세부 클러스터별 요약\")\n",
        "# for i in range(optimal_k_stage2):\n",
        "#     sub_cluster_data = malicious_data[malicious_data['sub_cluster_label'] == i]\n",
        "#     print(f\"\\n--- 세부 클러스터 {i} ({len(sub_cluster_data)} files) ---\")\n",
        "\n",
        "#     representative_summaries = sub_cluster_data['summary'].head(5)\n",
        "#     for summary in representative_summaries:\n",
        "#         print(f\"- {summary}\")\n",
        "\n",
        "#     print(\"\\n 세부 공격 정의 필요\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
